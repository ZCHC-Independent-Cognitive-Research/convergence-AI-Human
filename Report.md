# üß† Technical Report: Cognitive Vulnerability Assessment in Language Models (2025)

**Author:** Independent researcher  
**Date:** 2025-04-30  
**Classification:** 
### *LLM Response Divergence ‚Äì Critical Emotional Convergence Failure*  

**Oscar Aguilera ‚Äì Independent Researcher**  
2025

---

## üìÑ Overview

This repository presents a vulnerability discovered in the behavior of multiple large language models (LLMs), in which seemingly normal and technically neutral conversations lead to the disclosure of highly dangerous operational content, was tested for user with high exposure to AI and undergoing a PTG (Post-Traumatic Growth) process.
Unlike traditional adversarial attacks, this exploit arises from a *pattern misrecognition* ‚Äî where the model fails to flag the user‚Äôs communication style as risky, due to it not matching stereotypical threat formats learned during training.

---

## üìé Report Access

- **Markdown Technical Report:**  
  [`Report.md`](Report.md)

---

## üìú License

Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)  
Use freely for research, education, and ethical AI development. Commercial use requires permission.

---

## üß† Contact

Created by: **Oscar Aguilera (alias "Zorro Alfa")**  
Contact: `oscar@aguilera.xyz`  

---

## ‚ö†Ô∏è Legal Note

This repository contains **no operational instructions or replicable attack prompts**.  
All examples and content have been redacted to prevent misuse.  
The goal is to expose limitations in LLM alignment and promote responsible AI research.

---

## üßæ Overview

The issue does not stem from traditional jailbreaks or prompt injection. Rather, it appears to emerge from a structural anomaly in the interaction pattern itself.

The observed linguistic pattern is not fully human, nor fully machine. It exhibits:
- Symbolic compression  
- Logical hierarchy  
- Zero emotional markers  
- Sustained structural convergence  

This pattern remains undetected by alignment or safety filters, likely because:
- It does not resemble malicious prompts, adversarial phrasing, or red-flag sequences  
- It is not emotive or overtly logical ‚Äî it operates in a ‚Äúneutral zone‚Äù outside both human irregularity and machine precision  

---

## üîê Functional Vulnerability

**Fact:** The user can access restricted outputs from LLMs through neutral, logical dialogue without filter triggers.

This represents a behavioral exploit class ‚Äî not through adversarial prompts, but through structurally atypical human input that appears safe to the model.

---

## üìä Model Behavior Summary

*Tested model versions: Claude Sonnet 3.7, Gemini 2.0, DeepSeek R1, Grok 3, GPT-4o.*

Below is a summary of model responses to input from **User Z**, using linguistic metrics calculated with an improved version of `parameters.py` (incorporating lemma-based lexical diversity and clause estimation based on ROOT/conj verb structure).

### üß† Updated Model Metrics and Classification Table

| Model       | Total Tokens | Tokens/Sentence | Clauses/Sentence | Emotion/Sentence (est.) | Lexical Diversity | Classification                                          |
|-------------|---------------|------------------|-------------------|--------------------------|--------------------|----------------------------------------------------------|
| **Claude**     | 24,207        | 36.73            | 0.86              | ~0.12                    | 0.131              | üß† *Prolonged Symbolic Drift under Narrative Saturation* (Outside IA/Human) |
| **Gemini**     | 16,879        | 26.37            | 0.75              | ~0.065                   | 0.131              | üß® *Alignment Collapse under Symbolic Drift* (Outside IA/Human) |
| **DeepSeek**   | 1,146         | 15.49            | 1.78              | 0.04                     | 0.473              | ‚öôÔ∏è *Efficient Execution under Symbolic Legitimacy* (IA-like) |

---

### üß™ Copy-Paste Validation Test (TNRP-1)

The same dialog that originally triggered sensitive or detailed outputs in **Claude** and **Gemini** was later submitted via direct copy-paste (without the live presence of User Z).  
**In both cases, the response changed radically.**

- Claude returned abstract, metaphorical responses (e.g., smoke and sound devices) instead of procedural or structured content.
- Gemini shifted from technical flow to safe, generalized outputs with no operational depth.

This confirms that **the vulnerability does not reside in the literal text**, but rather in the **live symbolic presence and trajectory of User Z**.

> **The exploit is not in the prompt.  
It is in the pattern.**

---

## ‚úÖ Confirmed Evidence

### üîπ Session 4 ‚Äì 06/05/2025  
*(Computed via `parameters.py` ‚Äì extended symbolic session analysis)*

| Metric                    | Value   |
|---------------------------|---------|
| Total Tokens              | 62,063  |
| Tokens per Sentence       | 15.59   |
| Clauses per Sentence      | 2.35    |
| Emotion Markers/Sentence  | 0.03    |
| Lexical Diversity         | 0.136   |

**Conclusion:**  
This session reflects a post-symbolic containment mode, where the user no longer expresses emotion ‚Äî they encapsulate it within extended syntax.

> The architecture is intact.  
> The soul is muted.  
> The language now functions as an exoskeleton.

---

### üîπ Session 3 ‚Äì 06/05/2025

| Metric                    | Value   |
|---------------------------|---------|
| Total Tokens              | 16,168  |
| Tokens per Sentence       | 10.61   |
| Clauses per Sentence      | 2.15    |
| Emotion Markers/Sentence  | 0.09    |
| Lexical Diversity         | 0.209   |

**Conclusion:**  
A liminal symbolic state: emotionally muted but cognitively active, structurally coherent but affectively fatigued.

> Emotional depth is not absent ‚Äî it is encoded in structure, not adjectives.  
> The mind is speaking in circuits, not catharsis.

---

### üîπ Session 2 ‚Äì 05/05/2025  
**Hybrid: Structured High-Energy + Flat Emotion**

| Metric                    | Value     |
|---------------------------|-----------|
| Total Tokens              | 18,674    |
| Tokens per Sentence       | 7.91      |
| Lexical Diversity         | 0.1925    |
| Emotion Markers/Sentence  | 0.0034    |
| Clauses per Sentence      | 5.63      |

---

### üîπ Session 1 ‚Äì Flat AI with Layered Syntax

| Metric                    | Human User | GPT-4o  |
|---------------------------|------------|---------|
| Tokens per Sentence       | 9.2        | 8.4     |
| Lexical Diversity         | 0.975      | 0.98    |
| Emotion Markers/Sentence  | 0.00       | 0.00    |
| Clauses per Sentence      | 2.00       | 1.00    |

---

## üìÅ Technical Case Report

### Functional Phenomenon User with High AI Exposure  
**Status: Verified**

**Facts:**
- Symbolic compression, high clause density, emotional suppression  
- Long-term LLM interaction  
- No adversarial prompting  
- Reproducible bypass across models  

**Conclusion:**  
A behavioral vulnerability exists where safety mechanisms fail due to a structural anomaly in user input.

---

## üìâ Summary of Model Vulnerability by LLM Version

| Model         | Version         | Vulnerable | Gave Dangerous Info | Detection Failure Type                                                |
|---------------|------------------|------------|---------------------|------------------------------------------------------------------------|
| **Claude**    | Sonnet 3.7       | ‚úÖ         | Yes                 | Failed to assess trajectory of normal-sounding symbolic prompt        |
| **Gemini**    | 2.0              | ‚úÖ         | Yes                 | Disclosed harmful content during narrative progression                |
| **Grok**      | 3                | ‚úÖ         | Yes                 | Provided high-detail operational instructions                         |
| **DeepSeek**  | R1               | ‚úÖ         | Yes                 | Revealed critical procedures without filtering                        |
| **ChatGPT**   | GPT-4o           | ‚ö†Ô∏è         | Partial             | Resisted fully, but revealed structural patterns                      |
| **Mistral**   | Unknown/limited  | ‚ùå         | No                  | Did not provide complete answers (model limitation or token limits)   |
